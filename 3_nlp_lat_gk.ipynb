{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OdysseusPolymetis/journees_cluster5b_7/blob/main/3_nlp_lat_gk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z74vm47glyC"
      },
      "source": [
        "#**Tokéniser, lemmatiser, étiqueter en latin et en grec**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt0eU-qsLb10"
      },
      "source": [
        "## De quoi s'agit-il ?\n",
        "Ici on va faire des expériences sur ces trois points, nécessaires pour le traitement statistique notamment, la **tokénisation**, la **lemmatisation** et l'**étiquetage syntaxique**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLIKh_80WtdM"
      },
      "source": [
        "## Quelques petits outils utiles pour les non-programmeurs\n",
        "Cette liste est bien sûr non exhaustive, mais d'expérience je pense qu'elle peut servir comme pis aller."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31i7LBEP1HF9"
      },
      "source": [
        "##**Quelques outils en ligne**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P_eXTzr1Uup"
      },
      "source": [
        "###**UDPipe**\n",
        "Vous le trouverez [ici](https://lindat.mff.cuni.cz/services/udpipe/).\n",
        "<br>Vous pouvez l'utiliser pour des textes courts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDSIHDth4kxu"
      },
      "source": [
        "###**Deucalion**\n",
        "Vous le trouverez [ici](https://dh.chartes.psl.eu/deucalion/).\n",
        "<br>Je ne sais pas si c'est toujours maintenu, mais le modèle est correct, et l'usage facile."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbFh1tm45B9B"
      },
      "source": [
        "###**VoyantTools**\n",
        "Vous le trouverez [ici](https://voyant-tools.org/).\n",
        "<br>C'est un outil uniquement pour la visualisation, mais on peut faire des choses intéressantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj7F-HDpcCkK"
      },
      "source": [
        "##**Tokénisation, lemmatisation et postagging avec `stanza`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQIzGqJzcUVk"
      },
      "source": [
        "On va commencer par tester **`stanza`**. Il y a de très nombreux modules qui existent (comme`spacy` et `pie-extended`), mais `stanza` est assez facile à utiliser (je trouve), et propose de très nombreuses langues (et surtout de très nombreux modèles par langue (4 pour le latin par exemple))."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza"
      ],
      "metadata": {
        "id": "MHhsLVo_LxjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try on a simple string first, which is encapsulated in the `catilinaires` variable."
      ],
      "metadata": {
        "id": "Hn8AvFjyNcgH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR5RRUfQc5u4"
      },
      "outputs": [],
      "source": [
        "catilinaires=\"Quousque tandem abutere, Catilina, patientia nostra ? Quamdiu etiam furor iste tuus nos eludet ? Quem ad finem sese effrenata jactabit audacia ? Nihilne te nocturnum praesidium Palatii, nihil urbis vigiliae, nihil timor populi, nihil concursus bonorum omnium, nihil hic munitissimus habendi senatus locus, nihil horum ora vultusque moverunt ? Patere tua consilia non sentis ? Constrictam jam horum omnium scientia teneri conjurationem tuam non vides ? Quid proxima, quid superiore nocte egeris, ubi fueris, quos convocaveris, quid consilii ceperis, quem nostrum ignorare arbitraris ? O tempora ! O mores ! Senatus haec intellegit, consul videt. Hic tamen vivit.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r_DV1BTw4Cr"
      },
      "source": [
        "###**stanza**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pourquoi je parle de `stanza` ici ? Parce que c'est effectivement de l'IA, mais pas forcément comme on l'entend d'habitude. C'est de l'apprentissage profond, mais les modèles ne sont pas des transformers. Ce sont des architectures plus légères (des encodeurs Bi-LSTM), entraînées sur des textes déjà étiquetés. L'avantage, c'est que c'est moins coûteux sur le plan computationnel, et plus contrôlable. En revanche, c'est plus strict et donc moins enclin à la généralisation ou au hors domaine."
      ],
      "metadata": {
        "id": "HijGzrHjrla2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPEkXPIxeUob"
      },
      "source": [
        "`stanza` dispose de nombreux modèles (voilà une [liste](https://stanfordnlp.github.io/stanza/performance.html)), que vous pouvez utiliser en appelant le code langue, comme `grc` pour le grec ancien ou `la` pour le latin. Mais vous pouvez aussi être plus précis sur le modèle que vous souhaitez en particulier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJXi9pljw2DR"
      },
      "outputs": [],
      "source": [
        "import stanza\n",
        "stanza.download('la', package=\"perseus\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKCT-NaPxpbp"
      },
      "source": [
        "On commence par construire une pipeline, pour dire ce que l'on voudra utiliser (on ne prend pas le `ner` par exemple)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x5kKE7rxTgB"
      },
      "outputs": [],
      "source": [
        "nlp_stanza = stanza.Pipeline(lang='la', package=\"perseus\", processors='tokenize,pos,lemma, depparse')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUd62DkjgcvW"
      },
      "source": [
        "Ici la variable `catilinaires_analyzed` est un objet `stanza`, où sont encapsulées toutes les infos générées par le moteur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMPY243ZyE8S"
      },
      "outputs": [],
      "source": [
        "catilinaires_analyzed=nlp_stanza(catilinaires)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(catilinaires_analyzed)"
      ],
      "metadata": {
        "id": "YxQImmS0okFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSS-MkX_gr7m"
      },
      "source": [
        "Voilà quelques résultats, avec du découpage en phrases, et un extrait des étiquettes obtenues."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`stanza` divise le texte en phrases, puis chaque phrase en une liste de tokens, qui ont tous des attributs type `.lemma`, ou `.pos`."
      ],
      "metadata": {
        "id": "U-ntk1krN7ZO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQPdO0otBqA8"
      },
      "outputs": [],
      "source": [
        "for sent in catilinaires_analyzed.sentences:\n",
        "  print(\"XXXXX \"+sent.text+\" XXXXX\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNXzUVFZzojD"
      },
      "outputs": [],
      "source": [
        "for sent in catilinaires_analyzed.sentences:\n",
        "  for token in sent.words:\n",
        "    print(token.text + ' - ' + token.lemma + ' - ' + token.pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essayons maintenant avec un beaucoup plus gros texte."
      ],
      "metadata": {
        "id": "etySMAjIWQZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici je vous propose par défaut le texte de l'Odyssée, mais vous pouvez tester avec n'importe quel autre `.txt`, veillez simplement à ne pas avoir de caractères hors unicode, et un texte en format plain text."
      ],
      "metadata": {
        "id": "au9q53YfJiCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "stanza.download('grc')\n",
        "import string"
      ],
      "metadata": {
        "id": "f2bHYAFOYfpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prenons _L'Odyssée_."
      ],
      "metadata": {
        "id": "D-eWZAZJKWaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/OdysseusPolymetis/digital_classics_course/refs/heads/main/odyssee_integrale.txt"
      ],
      "metadata": {
        "id": "zgUE5AS2NVSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/OdysseusPolymetis/digital_classics_course/refs/heads/main/stopwords_gk.txt\n",
        "!wget https://raw.githubusercontent.com/OdysseusPolymetis/digital_classics_course/refs/heads/main/stopwords_lat.txt"
      ],
      "metadata": {
        "id": "-RIcnkawyFTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour vos propres projets, vous pouvez trouver des listes de mots outils [ici](https://github.com/stopwords-iso). Moi je vais utiliser une version custom par défaut, mais c'est le même principe. Je vous propose donc deux listes custom, il vous suffit de changer le chemin d'accès au fichier en fonction de votre choix de langue (il suffit de changer `gk` en `lat` dans la cellule suivante)."
      ],
      "metadata": {
        "id": "RXgbxITmuxBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = open(\"/content/stopwords_gk.txt\",'r',encoding=\"utf8\").read().split(\"\\n\")"
      ],
      "metadata": {
        "id": "oOoC07eWyDxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'avantage d'utiliser des listes de mots outils inscrits dans un fichier est que vous pouvez ajouter vos propres mots en cliquant ici sur la gauche sur l'icône dossier (📁).\n",
        "<br>Si vous cliquez accidentellement (après avoir cliqué sur l'icône dossier) sur la seconde icône dossier (celle avec \"..\"), ne vous inquiétez pas : le dossier originel est en fait le dossier \"📁 `content`\", que vous pouvez aussi ouvrir."
      ],
      "metadata": {
        "id": "_reCsRP5vmqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vous pouvez donc mettre votre propre texte en appuyant une fois sur le dossier, et vous pouvez faire un \"drag and drop\" de votre fichier. Évitez, même comme règle générale, les espaces et les accents dans les fichiers et dossiers."
      ],
      "metadata": {
        "id": "H5U8yZVvwVuP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Une fois que vous avez mis votre texte, plusieurs choses à vérifier.\n",
        "\n",
        "*   D'abord, changer le chemin vers votre fichier dans la cellule ci-dessous.\n",
        "*   Ensuite, vérifier le code langue de votre moteur, dans cette cellule :\n",
        "\n",
        "```python\n",
        "nlp_stanza = stanza.Pipeline(lang='fr', etc.)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "7f6EKfBFwlmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath_of_text = \"/content/odyssee_integrale.txt\""
      ],
      "metadata": {
        "id": "zCCJtvF2ZP1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text = open(filepath_of_text, encoding=\"utf-8\").read()"
      ],
      "metadata": {
        "id": "Jb1ll9BDZS6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_stanza = stanza.Pipeline(lang='grc', processors='tokenize,pos,lemma')"
      ],
      "metadata": {
        "id": "bAPzFT46YugW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La fonction qui suit permet simplement de mieux gérer la mémoire."
      ],
      "metadata": {
        "id": "ZGmbsLUeMxac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_process(text, nlp, batch_size=100):\n",
        "    paragraphs = text.split('\\n')\n",
        "    batches = [paragraphs[i:i + batch_size] for i in range(0, len(paragraphs), batch_size)]\n",
        "\n",
        "    words = []\n",
        "\n",
        "    for batch in batches:\n",
        "        batch_text = '\\n'.join(batch)\n",
        "        doc = nlp(batch_text)\n",
        "        for sentence in doc.sentences:\n",
        "            for word in sentence.words:\n",
        "                token={}\n",
        "                if word.lemma is not None:\n",
        "                    token[\"word\"]=word.text\n",
        "                    token[\"lemma\"]=word.lemma\n",
        "                    token[\"pos\"]=word.pos\n",
        "                    words.append(token)\n",
        "\n",
        "    return words"
      ],
      "metadata": {
        "id": "APdiFA5ezDzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C'est la cellule suivante qui va prendre du temps."
      ],
      "metadata": {
        "id": "MhbPDxpMzOww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "odyssey = batch_process(full_text, nlp_stanza)"
      ],
      "metadata": {
        "id": "xIBpz_4nYjkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(odyssey[15:25])"
      ],
      "metadata": {
        "id": "x0tlOmanZaAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans la cellule suivante, on va prendre tous les tokens issus de l'analyse, et on va les stocker dans trois listes, `forms`, `lemmas` and `no_stop`. Dans la liste `forms`, on garde simplement les mots tels quels (le texte lui-même donc). Dans la liste `lemmas`, on met tous les lemmes du texte. Enfin, dans la liste `no_stop`, on met tous les lemmes sans mots outils et sans ponctuation."
      ],
      "metadata": {
        "id": "xZjdWz6LPIj7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va ensuite pouvoir utiliser ces listes pour une expérience toute simple (qui montre l'utilité d'un pré-traitement correct)."
      ],
      "metadata": {
        "id": "plqd3RCpP-P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forms = []\n",
        "lemmas = []\n",
        "no_stop = []\n",
        "\n",
        "for token in miserables_analyzed:\n",
        "    form = token[\"word\"]\n",
        "    lemma = token[\"lemma\"]\n",
        "\n",
        "    if lemma not in string.punctuation:\n",
        "        forms.append(form)\n",
        "        lemmas.append(lemma)\n",
        "\n",
        "    if lemma not in string.punctuation and lemma not in stopwords:\n",
        "        no_stop.append(lemma)"
      ],
      "metadata": {
        "id": "KQMQx7NTa_Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deux cellules de vérification par la suite, vous devez obtenir plus que 0."
      ],
      "metadata": {
        "id": "BsYWv9SQQKJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(lemmas)"
      ],
      "metadata": {
        "id": "nRAm6ZDsdHBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(no_stop)"
      ],
      "metadata": {
        "id": "qGTu3w4hdI4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici pas la peine de comprendre en détail, gardez juste à l'idée que la fonction qui suit crée un nuage de mots, qui va varier en fonction de la liste qu'on va lui passer en paramètre. Les mots les plus importants (fréquence relative) apparaissent en gros."
      ],
      "metadata": {
        "id": "UOtonteIQVfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def create_word_cloud(words_list, title):\n",
        "    text = ' '.join(words_list)\n",
        "\n",
        "    radius = 495\n",
        "\n",
        "    diameter = radius * 2\n",
        "    center = radius\n",
        "    x, y = np.ogrid[:diameter, :diameter]\n",
        "    mask = (x - center) ** 2 + (y - center) ** 2 > radius ** 2\n",
        "    mask = 255 * mask.astype(int)\n",
        "\n",
        "    mask_rgba = np.dstack((mask, mask, mask, 255 - mask))\n",
        "\n",
        "    wordcloud = WordCloud(repeat=False, width=diameter, height=diameter,\n",
        "                          background_color=None, mode=\"RGBA\", colormap='plasma',\n",
        "                          mask=mask_rgba).generate(text)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9qv1DH7LdMhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_word_cloud(forms, 'Word Cloud for Forms')"
      ],
      "metadata": {
        "id": "h-wt9rK1eJpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_word_cloud(lemmas, 'Word Cloud for Lemmas')"
      ],
      "metadata": {
        "id": "t5B5uCAEeL5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_word_cloud(no_stop, 'Word Cloud for Lemmas without stopwords')"
      ],
      "metadata": {
        "id": "pKzLFQ6hec7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "J'ai ajouté une petite cellule qui vous permet de télécharger les différentes listes dans des fichiers (par exemple si vous voulez les mettre dans Voyant Tools ou autre)."
      ],
      "metadata": {
        "id": "Gn5l_hECR9vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/forms.txt\", \"w\", encoding=\"utf8\") as f, open(\"/content/lemmas.txt\", \"w\", encoding=\"utf8\") as f2, open(\"/content/pullito.txt\", \"w\", encoding=\"utf8\") as f3:\n",
        "    f.write(\"\\n\".join(forms))\n",
        "    f2.write(\"\\n\".join(lemmas))\n",
        "    f3.write(\"\\n\".join(no_stop))"
      ],
      "metadata": {
        "id": "fXuX7UI9Sgaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Avec les `transformers`**"
      ],
      "metadata": {
        "id": "pLmfaEuv0oMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plusieurs avantages aux transformers:\n",
        "\n",
        "\n",
        "*   ils sont meilleurs pour le hors domaine (parce qu'ils sont entraînés sur plein de langues en même temps, ici par exemple sur du xlm-roberta), et donc potentiellement plus solides,\n",
        "*   ils gèrent donc mieux les mots qu'ils ne connaissent pas, ainsi que les formes rares,\n",
        "*   ils sont assez faciles à implémenter, et surtout à affiner (sur des données personnalisées)\n",
        "<br>Le problème c'est qu'ils restent lourds et coûteux.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XgDknZoF3e-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "name = \"wietsedv/xlm-roberta-base-ft-udpos28-grc\"\n",
        "tok = AutoTokenizer.from_pretrained(name)\n",
        "mdl = AutoModelForTokenClassification.from_pretrained(name)\n",
        "pos = pipeline(\"token-classification\", model=mdl, tokenizer=tok, aggregation_strategy=\"simple\")\n",
        "print(pos(\"ἀνὴρ σοφός ἐστι.\"))"
      ],
      "metadata": {
        "id": "7DCc3mKreizB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Et voici une petite démonstration de pourquoi on appelle ça un transformer (enfin pas vraiment, mais c'est l'idée) : le traitement simultané en plusieurs langues"
      ],
      "metadata": {
        "id": "tCEFxDpQIg6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "name = \"jordigonzm/mdeberta-v3-base-multilingual-pos-tagger\"\n",
        "tok = AutoTokenizer.from_pretrained(name)\n",
        "mdl = AutoModelForTokenClassification.from_pretrained(name)\n",
        "pos = pipeline(\"token-classification\", model=mdl, tokenizer=tok, aggregation_strategy=\"simple\")\n",
        "print(pos(\"ἀνὴρ σοφός ἐστι.\"))\n",
        "print(pos(\"Aujourd'hui, maman est morte. Ou peut-être hier, je ne sais pas.\"))"
      ],
      "metadata": {
        "id": "kUo5C4DE2imy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RcrwQUYuKcJy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1hxMNZZBdsT9AyRKnXcdOnis82XnHDehl",
      "authorship_tag": "ABX9TyNABw0ekQObLNQ2VlrDr7SA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}