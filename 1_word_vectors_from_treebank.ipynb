{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmAHSU8Y4mLC8yBID7TNB7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OdysseusPolymetis/journees_cluster5b_7/blob/main/1_word_vectors_from_treebank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Vecteurs de mots statiques pour le grec ancien**\n",
        "---\n"
      ],
      "metadata": {
        "id": "F44-RzJtRAUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans ce notebook, vous pourrez prendre le texte de votre choix (grec ou latin, mais limité) ici : [Perseus Treebank](https://github.com/PerseusDL/treebank_data.git)."
      ],
      "metadata": {
        "id": "Bwo5xJSBRdnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons utiliser cette banque parce que la plupart des données a été à peu près contrôlée, et correctement lemmatisée. Ça nous évitera le traitement par des modules tiers."
      ],
      "metadata": {
        "id": "S-mDSxm0RzEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Préparation des phrases\n"
      ],
      "metadata": {
        "id": "FYrqm1RpbkL1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i91dUjI1Q0vV",
        "outputId": "7ab7ce9a-5947-4f35-c27c-f2aaf6ce95f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'treebank_data'...\n",
            "remote: Enumerating objects: 2779, done.\u001b[K\n",
            "remote: Counting objects: 100% (368/368), done.\u001b[K\n",
            "remote: Compressing objects: 100% (225/225), done.\u001b[K\n",
            "remote: Total 2779 (delta 222), reused 217 (delta 142), pack-reused 2411 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2779/2779), 75.66 MiB | 24.46 MiB/s, done.\n",
            "Resolving deltas: 100% (1577/1577), done.\n",
            "Updating files: 100% (276/276), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/PerseusDL/treebank_data.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "J'ajoute une liste de mots outils : décommentez la commande dont vous avez besoin en fonction de votre choix de langue."
      ],
      "metadata": {
        "id": "c0tM6TShVXfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/OdysseusPolymetis/digital_classics_course/main/stopwords_gk.txt\n",
        "#!wget https://raw.githubusercontent.com/OdysseusPolymetis/digital_classics_course/main/stopwords_lat.txt"
      ],
      "metadata": {
        "id": "SMk-5BXFShiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "import glob"
      ],
      "metadata": {
        "id": "Lnw6KdksViQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deux fonctions à suivre, une qui charge les mots outils (attention au chemin de fichier quand vous l'appellerez), et l'autre qui parcourt les dossiers (même remarque)."
      ],
      "metadata": {
        "id": "Y1VACp6WaBWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_stopwords(path_txt):\n",
        "    with open(path_txt, \"r\", encoding=\"utf-8\") as f:\n",
        "        return {line.strip() for line in f if line.strip() and not line.lstrip().startswith(\"#\")}"
      ],
      "metadata": {
        "id": "U8dFFRG4aMfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans cette fonction, vous avez l'option `stopwords`, que vous pourrez activer ou non à l'appel (par défaut ce sera activé)."
      ],
      "metadata": {
        "id": "pGvBKUC9bBPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmas_by_sentence(xml_path, include_punct=False, stopwords=None):\n",
        "    xml_path = Path(xml_path)\n",
        "    sentences = []\n",
        "    current = []\n",
        "\n",
        "    for event, elem in ET.iterparse(xml_path, events=(\"start\", \"end\")):\n",
        "        tag = elem.tag\n",
        "\n",
        "        if event == \"start\" and tag == \"sentence\":\n",
        "            current = []\n",
        "\n",
        "        elif event == \"end\" and tag == \"word\":\n",
        "            postag = elem.attrib.get(\"postag\", \"\")\n",
        "            if not include_punct and postag.startswith(\"u\"):\n",
        "                pass\n",
        "            else:\n",
        "                lemma = elem.attrib.get(\"lemma\")\n",
        "                if lemma:\n",
        "                    if not stopwords or lemma not in stopwords:\n",
        "                        current.append(lemma)\n",
        "\n",
        "        elif event == \"end\" and tag == \"sentence\":\n",
        "            sentences.append(current)\n",
        "            elem.clear()\n",
        "\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "tbusR3r1VoPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour la première variable `text_folder`, vous devez mettre le chemin exact qui mène au dossier auteur (si vous voulez traiter un auteur en entier) ou au dossier texte. La seconde variable `auteur` devra contenir le code auteur de votre choix (code urn sur phi ou tlg, exemple : pour Homère, tlg0012)."
      ],
      "metadata": {
        "id": "4wGpv0zTWVUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_folder = \"/content/treebank_data/v2.1/Greek/texts\"\n",
        "auteur = \"tlg0012\""
      ],
      "metadata": {
        "id": "PLjEpu-9V5h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici, selon que vous utilisez le grec ou le latin, décommentez la ligne dont vous avez besoin, commentez l'autre."
      ],
      "metadata": {
        "id": "ZnJsBooHaeLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = load_stopwords(\"/content/stopwords_gk.txt\")\n",
        "#stopwords = load_stopwords(\"/content/stopwords_lat.txt\")"
      ],
      "metadata": {
        "id": "GpRaxNSOaYpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = sorted(glob.glob(os.path.join(text_folder, \"**\", f\"{auteur}.*.xml\"), recursive=True))"
      ],
      "metadata": {
        "id": "_89gIvmQXw0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [sentence for f in files for sentence in lemmas_by_sentence(f, stopwords=stopwords)]"
      ],
      "metadata": {
        "id": "Cy_t733cZDWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On vérifie simplement que les phrases sont bien prises en charge :"
      ],
      "metadata": {
        "id": "B3S5cA2wZV01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZYLFWS4ZE_9",
        "outputId": "657c3044-9d59-4320-b123-96bc8401cdc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15138"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Vectorisation"
      ],
      "metadata": {
        "id": "YhcwYrLbbx69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour effectuer la vectorisation, nous allons passer par une librairie classique, gensim (dispo [ici](https://radimrehurek.com/gensim/))."
      ],
      "metadata": {
        "id": "u99UUlLHcHKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "jpytAajfcA9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "Oj5E_ymUZalW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La cellule qui suit va permettre de constituer le modèle. Le temps que ça prendra dépendra des paramètres que vous mettrez. Voilà une brève explication.\n",
        "\n",
        "\n",
        "*   `sentences` : c'est la liste de phrases lemmatisées\n",
        "*   `min_count` : c'est le nombre minimal de fois qu'un mot doit apparaître pour être pris en compte\n",
        "*   `max_vocab_size` : c'est le nombre de mots max qui va être compris dans le modèle\n",
        "*   `negative` : le modèle voit des paires vraies (mot en contexte) et, pour chaque paire vraie, il voit aussi des paires fausses (ce qu'on appelle adversarial training) choisies au hasard, ici X faux voisins par vrai voisin : théoriquement, plus cette valeur est haute, plus l’apprentissage est précis, et coûteux\n",
        "*   `epochs` : nombre de fois où le modèle parcourt tout le corpus\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ccrno3H-cm6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences, min_count=2, max_vocab_size=10000, negative=50, epochs=300)"
      ],
      "metadata": {
        "id": "Sl58Ui1Sb7lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(positive=[\"γυνή\",\"θεός\"], negative=[\"ἀνήρ\"],topn=10)"
      ],
      "metadata": {
        "id": "NJT31LDBcmPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('θεός',topn=20)"
      ],
      "metadata": {
        "id": "-Z87y2zrfCp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Et voici un petit bout de code pour exporter vos données et les visualiser plus aisément. Vous pouvez vous rendre sur le site de projection de [tensorflow](https://projector.tensorflow.org/), appuyer sur le bouton `load` sur la gauche, mettre le fichier de `1_vecteurs.tsv` en première option du pop-up, et le fichier `2_metadonnees.tsv` dans la seconde."
      ],
      "metadata": {
        "id": "RHvAis3FhjKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/1_vecteurs.tsv\", 'w') as file_vectors, open(\"/content/2_metadonnees.tsv\", 'w') as file_metadata:\n",
        "    for word in model.wv.index_to_key:\n",
        "        file_vectors.write('\\t'.join([str(x) for x in model.wv[word]]) + \"\\n\")\n",
        "        file_metadata.write(word + \"\\n\")"
      ],
      "metadata": {
        "id": "Ry221Oc2hLpj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9hsImc2kik4w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}