{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMpZzj6wjrK04RbsTLcSPUR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OdysseusPolymetis/journees_cluster5b_7/blob/main/4_ner_lat_gk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reconnaissance d'entités nommées en latin et en grec**\n",
        "---"
      ],
      "metadata": {
        "id": "fI10jLtHiWWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Une entité nommée est une expression linguistique référentielle : pour simplifier, il s'agit d'entités culturellement reconnaissables et référençables. Exemple : Rome est une `LOC`, César est un `PER`, etc."
      ],
      "metadata": {
        "id": "iJJF3UQJie6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il existe de très nombreux modèles pour les langues modernes, c'est beaucoup moins fréquent pour les langues anciennes. Les premiers modèles NER sur transformers ont été ciblés par langue (exemple, le modèle d'Ugarit), mais de plus en plus on s'oriente vers des modèles massifs multilingues, qui comprennent aussi le grec ancien (Roberta large comprend maintenant le grec ancien, plutôt classique)."
      ],
      "metadata": {
        "id": "5v_afZ8Wi9at"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTR7n9VCfFYk"
      },
      "outputs": [],
      "source": [
        "!pip install flair"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "from transformers import pipeline\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata as ud\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "836CbgOtfLNa"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Tests sur phrases courtes**"
      ],
      "metadata": {
        "id": "elez6386j4JO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons d'abord tester sur deux phrases, mais vous pouvez faire le test sur d'autres choses. Nous le ferons sur des textes entiers plus tard."
      ],
      "metadata": {
        "id": "4D6xkquBkYca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_gk = 'ταῦτα εἴπας ὁ Ἀλέξανδρος παρίζει Πέρσῃ ἀνδρὶ ἄνδρα Μακεδόνα ὡς γυναῖκα τῷ λόγῳ · οἳ δέ , ἐπείτε σφέων οἱ Πέρσαι ψαύειν ἐπειρῶντο , διεργάζοντο αὐτούς .'\n",
        "sentence_lat = 'Quo usque tandem abutere, Catilina, patientia nostra ?'"
      ],
      "metadata": {
        "id": "-jTpZclZhhsf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici un premier modèle, assez ancien (2022), de l'équipe UGARIT. Petite explication des étiquettes :\n",
        "*   **O** : Outside, pas d'entité reconnaissable\n",
        "*   **S-PER** : Personnage/Personne, Single, l'entité est faite d'un seul mot\n",
        "*   **B-PER** : Personnage/Personne, Begin, le mot est le début d'une entité\n",
        "*   **E-PER** : Personnage/Personne, End, dernier mot de l'entité\n",
        "*   **I-PER** : Personnage/Personne, Inside, mot au milieu de l'entité\n",
        "*   **S-MISC** : Miscalleneous (ni `PER` ni `LOC`), Single, mot seul\n",
        "*   **B-MISC** : Miscalleneous (ni `PER` ni `LOC`), Begin, début de l'entité\n",
        "*   **E-MISC** : Miscalleneous (ni `PER` ni `LOC`), End, dernier mot de l'entité\n",
        "*   **I-MISC** : Miscalleneous (ni `PER` ni `LOC`), Inside, mot au milieu de l'entité\n",
        "*   **S-LOC** : Location, Single, l'entité est faite d'un seul mot\n",
        "*   **B-LOC** : Location, Begin, début de l'entité\n",
        "*   **E-LOC** : Location, End, fin de l'entité\n",
        "*   **I-LOC** : Location, Inside, mot au milieu de l'entité\n",
        "*   **\\<START>** : marqueur crf pour le modèle (début)\n",
        "*   **\\<STOP>** : marqueur crf pour le modèle (fin)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DLl8XVvgknQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagger = SequenceTagger.load(\"UGARIT/flair_grc_bert_ner\")"
      ],
      "metadata": {
        "id": "c1WQPBTNj1zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = Sentence('ταῦτα εἴπας ὁ Ἀλέξανδρος παρίζει Πέρσῃ ἀνδρὶ ἄνδρα Μακεδόνα ὡς γυναῖκα τῷ λόγῳ · οἳ δέ , ἐπείτε σφέων οἱ Πέρσαι ψαύειν ἐπειρῶντο , διεργάζοντο αὐτούς .')\n",
        "tagger.predict(sent)\n",
        "for entity in sent.get_spans('ner'):\n",
        "    print(entity)"
      ],
      "metadata": {
        "id": "JONmTtjifwKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essayons maintenant avec un très gros modèle multilingue (plus de 100 langues). Il s'agit d'un modèle plus récent (2023, je n'ai pas trouvé de modèle de NER multilingue (non finetuné sur une langue en particulier) plus récent)."
      ],
      "metadata": {
        "id": "NyX7AOK2oPqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner = pipeline(\n",
        "    \"token-classification\",\n",
        "    model=\"51la5/roberta-large-NER\",\n",
        "    aggregation_strategy=\"simple\"\n",
        ")"
      ],
      "metadata": {
        "id": "89xidiW7gAQ_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok = ner.tokenizer"
      ],
      "metadata": {
        "id": "SMH4Qa0YuALY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ner(sentence_gk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNazLcNJgepm",
        "outputId": "57206ba8-4442-422f-b475-f04d0cbba861"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'entity_group': 'PER', 'score': np.float32(0.97942376), 'word': 'Ἀλέξανδρος', 'start': 14, 'end': 24}, {'entity_group': 'MISC', 'score': np.float32(0.8012814), 'word': 'Πέρσ', 'start': 33, 'end': 37}, {'entity_group': 'MISC', 'score': np.float32(0.9995103), 'word': 'Μακεδόνα', 'start': 51, 'end': 59}, {'entity_group': 'MISC', 'score': np.float32(0.99963826), 'word': 'Πέρσαι', 'start': 105, 'end': 111}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ner(sentence_lat))"
      ],
      "metadata": {
        "id": "PPf9KqIlhbJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Et faisons un test absurde, mélangeons deux langues en une phrase !"
      ],
      "metadata": {
        "id": "jqbfmcl2pOVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mix = 'Quo usque tandem abutere, ὦ Ἀλέξανδρε, patientia nostra ?'\n",
        "print(ner(mix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9CUf5SRiOYh",
        "outputId": "4b448e72-27b3-4462-f6d2-d586042744f1"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'entity_group': 'PER', 'score': np.float32(0.89059514), 'word': 'Ἀλέξανδρε', 'start': 28, 'end': 37}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _Pourquoi ça marche ?_\n",
        "En fait il faut partir de l'idée que ça fonctionnerait sur n'importe quelle langue sur laquelle il a été entraîné : toutes les langues sont mappées sur un seul espace sémantique, pour lui il n'existe pas vraiment des langues, mais une langue, une représentation sémantique globale."
      ],
      "metadata": {
        "id": "l21F5GPKpsGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Long texte et représentation graphique**"
      ],
      "metadata": {
        "id": "66Qook5Gq_5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**⚠** Je le précise tout de suite, le résultat ne peut pas être parfait. Il y aura forcément des erreurs, d'autant que les données annotées sont limitées."
      ],
      "metadata": {
        "id": "DnAlvKMerGKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons reprendre l'_Odyssée_, en la téléchargeant dans la cellule suivante. Mais vous pouvez mettre votre propre texte (latin ou grec, nous allons utiliser le gros modèle, et ça prendra du temps)."
      ],
      "metadata": {
        "id": "8mAAqyczsBiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/OdysseusPolymetis/digital_classics_course/refs/heads/main/odyssee_integrale.txt"
      ],
      "metadata": {
        "id": "0Gq6LrFOpmW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath_of_text = \"/content/odyssee_integrale.txt\""
      ],
      "metadata": {
        "id": "9QoZD0eesJIh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text = open(filepath_of_text, encoding=\"utf-8\").read()"
      ],
      "metadata": {
        "id": "AoFxM1VIsWAK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici on va télécharger les deux modèles, pour la lemmatisation, en latin et en grec."
      ],
      "metadata": {
        "id": "AdQVlzfruJkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stanza.download('la', verbose=False)\n",
        "stanza.download('grc', verbose=False)\n",
        "lemma_la  = stanza.Pipeline('la',  processors='tokenize,lemma', use_gpu=True)\n",
        "lemma_grc = stanza.Pipeline('grc', processors='tokenize,lemma', use_gpu=True)"
      ],
      "metadata": {
        "id": "yvQ_jvfl2zBc"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici on va donner les grandes variables dont on aura besoin ensuite (c'est plus clair de les mettre dès le début)"
      ],
      "metadata": {
        "id": "6eFqoyNlBuOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME       = \"51la5/roberta-large-NER\"\n",
        "WORDS_PER_CHUNK  = 100\n",
        "STRIDE_WORDS     = 50\n",
        "WINDOW_SIZE      = 20\n",
        "MIN_FREQ         = 5\n",
        "EDGE_MIN         = 1\n",
        "USE_GPU_STANZA   = True\n",
        "MIN_WORD_LEN     = 6"
      ],
      "metadata": {
        "id": "pJw3DE6Z9yyo"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORD_RE        = re.compile(r\"[^\\W\\d_]+\", re.UNICODE)\n",
        "GREEK_RE       = re.compile(r'[\\u0370-\\u03FF\\u1F00-\\u1FFF]')\n",
        "ZERO_WIDTH_RE  = re.compile(r\"[\\u200B-\\u200D\\uFEFF]\")\n",
        "WS_MULTI_RE    = re.compile(r\"\\s+\")"
      ],
      "metadata": {
        "id": "2NcBsC4H-cZy"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonction qui permet la normalisation unicode (pour le grec)"
      ],
      "metadata": {
        "id": "FuyKteZUB1n5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_unicode(s: str) -> str:\n",
        "    return ud.normalize(\"NFC\", s)"
      ],
      "metadata": {
        "id": "n4QLFTV8AAzd"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokénisation toute bête."
      ],
      "metadata": {
        "id": "s6yuMfaHB6-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_words(text: str):\n",
        "    return [(m.group(0), m.start(), m.end()) for m in WORD_RE.finditer(text)]"
      ],
      "metadata": {
        "id": "4Ax7RB7nAC6z"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Détection de la langue en fonction de la regex de grec."
      ],
      "metadata": {
        "id": "z9Np7WlzB9a1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_lang_by_script(s: str) -> str:\n",
        "    return \"grc\" if GREEK_RE.search(normalize_unicode(s)) else \"la\""
      ],
      "metadata": {
        "id": "3Sj3RETPAEq_"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonction de lemmatisation de petites portions de texte après traitement par le NER."
      ],
      "metadata": {
        "id": "eSv08ZfOCBVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_span(text: str) -> str:\n",
        "    text = normalize_unicode(text.strip())\n",
        "    nlp  = lemma_grc if detect_lang_by_script(text) == \"grc\" else lemma_la\n",
        "    doc  = nlp(text)\n",
        "    lemmas = []\n",
        "    for sent in doc.sentences:\n",
        "        for w in sent.words:\n",
        "            if re.fullmatch(r\"\\W+\", w.text or \"\"):\n",
        "                continue\n",
        "            lemmas.append(w.lemma or w.text)\n",
        "    return \" \".join(lemmas) if lemmas else text"
      ],
      "metadata": {
        "id": "7DcwZ4_TAHw1"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Petite fonction qui permet juste de lancer la lemmatisation des labels"
      ],
      "metadata": {
        "id": "2kLBbuXzCHxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def canonicalize_entity(surface: str) -> str:\n",
        "    return lemmatize_span(surface)"
      ],
      "metadata": {
        "id": "4Js6yF91AKpg"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokénisation et fenêtres glissantes."
      ],
      "metadata": {
        "id": "0aCZ6elGCN_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iter_word_chunks(text: str, words_per_chunk=WORDS_PER_CHUNK, stride_words=STRIDE_WORDS):\n",
        "    toks = tokenize_words(text)\n",
        "    if not toks: return\n",
        "    n, i = len(toks), 0\n",
        "    while i < n:\n",
        "        j = min(n, i + words_per_chunk)\n",
        "        yield toks[i][1], text[toks[i][1]:toks[j-1][2]]\n",
        "        if j == n: break\n",
        "        i += stride_words"
      ],
      "metadata": {
        "id": "2F5AuAuJAP45"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _yield_token_safe_slices(sub: str, base_char: int, tokenizer, max_tokens: int, overlap_words: int = 10):\n",
        "    words = tokenize_words(sub)\n",
        "    n, i = len(words), 0\n",
        "    while i < n:\n",
        "        j, last_ok = i+1, None\n",
        "        while j <= n:\n",
        "            piece = sub[words[i][1]:words[j-1][2]]\n",
        "            if len(tokenizer(piece)[\"input_ids\"]) <= max_tokens:\n",
        "                last_ok = j; j += 1\n",
        "            else:\n",
        "                break\n",
        "        if last_ok is None:\n",
        "            start_c = words[i][1]; end_c = min(len(sub), start_c + 2000)\n",
        "            yield base_char + start_c, base_char + end_c, sub[start_c:end_c]\n",
        "            i += 1\n",
        "        else:\n",
        "            start_c = words[i][1]; end_c = words[last_ok-1][2]\n",
        "            yield base_char + start_c, base_char + end_c, sub[start_c:end_c]\n",
        "            i = max(last_ok - overlap_words, last_ok)"
      ],
      "metadata": {
        "id": "YQzDbVbcAStO"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stockage des entités nommées qui nous intéressent (ici `PER`, mais ça peut être autre chose)."
      ],
      "metadata": {
        "id": "p9q7fYh1CSU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_entities(text: str, keep_labels={\"PER\"}, words_per_chunk=WORDS_PER_CHUNK, stride_words=STRIDE_WORDS, tokenizer=None, ner=None):\n",
        "    assert tokenizer is not None and ner is not None\n",
        "    text = normalize_unicode(text)\n",
        "    seen, out = set(), []\n",
        "    model_max = getattr(tokenizer, \"model_max_length\", 512)\n",
        "    safe_max  = min(510, model_max - 2)\n",
        "\n",
        "    for base, sub in iter_word_chunks(text, words_per_chunk, stride_words):\n",
        "        enc_len = len(tokenizer(sub)[\"input_ids\"])\n",
        "        pieces  = _yield_token_safe_slices(sub, base, tokenizer, safe_max) if enc_len > safe_max else [(base, base+len(sub), sub)]\n",
        "        for abs_start, _, piece in pieces:\n",
        "            for e in ner(piece):\n",
        "                label = e.get(\"entity_group\") or e.get(\"entity\")\n",
        "                if keep_labels and label not in keep_labels:\n",
        "                    continue\n",
        "                start = abs_start + int(e[\"start\"])\n",
        "                end   = abs_start + int(e[\"end\"])\n",
        "                key = (start, end, label)\n",
        "                if key in seen:\n",
        "                    continue\n",
        "                seen.add(key)\n",
        "                out.append({\"text\": e[\"word\"], \"label\": label, \"start\": start, \"end\": end})\n",
        "    return out"
      ],
      "metadata": {
        "id": "PzDdPREhAZ4y"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rassemblement des entités"
      ],
      "metadata": {
        "id": "fu9k1NSOCa05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attach_token_index_to_entities(text: str, entities):\n",
        "    words = tokenize_words(text)\n",
        "    if not words: return entities, words\n",
        "    for ent in entities:\n",
        "        s, i = ent[\"start\"], 0\n",
        "        while i < len(words) and words[i][2] <= s: i += 1\n",
        "        ent[\"tok_idx\"] = i if (i < len(words) and words[i][1] <= s < words[i][2]) else min(max(i,0), len(words)-1)\n",
        "    return entities, words"
      ],
      "metadata": {
        "id": "gBczlVSJAf8_"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calcul des cooccurences"
      ],
      "metadata": {
        "id": "Y5jTIBzaCeP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cooccurrence(entities, window_size_words=WINDOW_SIZE, key_fn=lambda e: e[\"text\"]):\n",
        "    co = defaultdict(lambda: defaultdict(int))\n",
        "    ents = sorted(entities, key=lambda e: e[\"tok_idx\"])\n",
        "    for i, ei in enumerate(ents):\n",
        "        a, ai = key_fn(ei), ei[\"tok_idx\"]\n",
        "        j = i + 1\n",
        "        while j < len(ents) and ents[j][\"tok_idx\"] - ai <= window_size_words:\n",
        "            b = key_fn(ents[j])\n",
        "            if a != b:\n",
        "                co[a][b] += 1\n",
        "                co[b][a] += 1\n",
        "            j += 1\n",
        "    return co"
      ],
      "metadata": {
        "id": "SYTLUB6zAj6Y"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Création de la matrice de cooccurrence"
      ],
      "metadata": {
        "id": "AeDPRKsBCi0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cooccurrence_to_matrix(co):\n",
        "    nodes = set(co.keys())\n",
        "    for a in co: nodes.update(co[a].keys())\n",
        "    nodes = sorted(nodes)\n",
        "    idx = {n:i for i,n in enumerate(nodes)}\n",
        "    M = np.zeros((len(nodes), len(nodes)), dtype=int)\n",
        "    for a, nbrs in co.items():\n",
        "        i = idx[a]\n",
        "        for b, w in nbrs.items():\n",
        "            M[i, idx[b]] = int(w)\n",
        "    return nodes, M"
      ],
      "metadata": {
        "id": "fODRBv85AoTQ"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Effacement des diacritiques (qui ne sont pas bons pour rassembler les labels)"
      ],
      "metadata": {
        "id": "X0ruHzKtClyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_diacritics(s: str) -> str:\n",
        "    s = ud.normalize(\"NFD\", s)\n",
        "    s = \"\".join(ch for ch in s if ud.category(ch) != \"Mn\")\n",
        "    return ud.normalize(\"NFC\", s)"
      ],
      "metadata": {
        "id": "1Fw3sC9cArwq"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_for_match(s: str) -> str:\n",
        "    s = strip_diacritics(s)\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s"
      ],
      "metadata": {
        "id": "A-uXfr_NAuHu"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quand des labels ont trois ou moins caractères qui se suivent qui diffèrent (pour les mots de plus de huit caractères), on les rassemble."
      ],
      "metadata": {
        "id": "0dh1dkTDCrl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def contiguous_diff_leq(s1: str, s2: str, max_run: int = 3) -> bool:\n",
        "    if s1 == s2:\n",
        "        return True\n",
        "    p = 0\n",
        "    L1, L2 = len(s1), len(s2)\n",
        "    while p < L1 and p < L2 and s1[p] == s2[p]:\n",
        "        p += 1\n",
        "    q = 0\n",
        "    while q < (L1 - p) and q < (L2 - p) and s1[L1-1-q] == s2[L2-1-q]:\n",
        "        q += 1\n",
        "    m1 = s1[p:L1-q] if p + q <= L1 else \"\"\n",
        "    m2 = s2[p:L2-q] if p + q <= L2 else \"\"\n",
        "    return max(len(m1), len(m2)) <= max_run"
      ],
      "metadata": {
        "id": "t4cN4zSoAwUt"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rassemblement des labels après tri par distance de caractères"
      ],
      "metadata": {
        "id": "4wxOjcVPC2kH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_merge_map(keys, freq: Counter, min_len: int = 6, max_run: int = 3):\n",
        "    order = sorted(keys, key=lambda k: (-freq[k], k))\n",
        "    rep_map, reps = {}, []\n",
        "    norm = {k: normalize_for_match(k) for k in order}\n",
        "    for k in order:\n",
        "        if k in rep_map:\n",
        "            continue\n",
        "        rep = k\n",
        "        reps.append(rep)\n",
        "        rep_map[rep] = rep\n",
        "        rep_norm = norm[rep]\n",
        "        for t in order:\n",
        "            if t in rep_map:\n",
        "                continue\n",
        "            t_norm = norm[t]\n",
        "            same_norm = (rep_norm == t_norm)\n",
        "            long_and_close = (len(rep_norm) >= min_len and len(t_norm) >= min_len and\n",
        "                              contiguous_diff_leq(rep_norm, t_norm, max_run=max_run))\n",
        "            if same_norm or long_and_close:\n",
        "                rep_map[t] = rep\n",
        "    return rep_map"
      ],
      "metadata": {
        "id": "4tVEfZpIA05h"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nettoyage des labels"
      ],
      "metadata": {
        "id": "WLJZBrwQC69m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_label(s: str) -> str:\n",
        "    s = normalize_unicode(s)\n",
        "    s = ZERO_WIDTH_RE.sub(\"\", s)\n",
        "    s = s.replace(\"\\r\",\" \").replace(\"\\n\",\" \").replace(\"\\t\",\" \")\n",
        "    return WS_MULTI_RE.sub(\" \", s).strip()"
      ],
      "metadata": {
        "id": "d8rFzzGcA403"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_id(label: str) -> str:\n",
        "    base = clean_label(label)\n",
        "    if not base: return \"\"\n",
        "    return \"n_\" + hashlib.sha1(base.encode(\"utf-8\")).hexdigest()[:12]"
      ],
      "metadata": {
        "id": "7dMF6NcMA7Pk"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Création du graphe pour Gephi"
      ],
      "metadata": {
        "id": "bZeJpZ4GC9ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_graph_from_matrix_safe(entities, M, edge_min=EDGE_MIN):\n",
        "    G = nx.Graph()\n",
        "    id_of = {}\n",
        "    for lab in entities:\n",
        "        nid = make_id(lab)\n",
        "        if not nid: continue\n",
        "        id_of[lab] = nid\n",
        "        G.add_node(nid, label=clean_label(lab))\n",
        "    n = len(entities)\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            w = int(M[i, j])\n",
        "            if w >= edge_min:\n",
        "                a, b = id_of.get(entities[i], \"\"), id_of.get(entities[j], \"\")\n",
        "                if a and b and a != b:\n",
        "                    G.add_edge(a, b, weight=w, label=str(w))\n",
        "    return G"
      ],
      "metadata": {
        "id": "xMz93s6KA9h0"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attributs pour Gephi"
      ],
      "metadata": {
        "id": "_EQDb6VPDGKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def enrich_and_write_gexf(G, path=\"network.gexf\"):\n",
        "    strength = dict(G.degree(weight=\"weight\"))\n",
        "    btw = nx.betweenness_centrality(G, weight=\"weight\", normalized=True) if G.number_of_edges() else {n:0.0 for n in G}\n",
        "    try:\n",
        "        eig = nx.eigenvector_centrality_numpy(G, weight=\"weight\") if G.number_of_edges() else {n:0.0 for n in G}\n",
        "    except Exception:\n",
        "        eig = {n:0.0 for n in G}\n",
        "\n",
        "    for n_ in G.nodes():\n",
        "        G.nodes[n_][\"strength\"]    = float(strength.get(n_, 0))\n",
        "        G.nodes[n_][\"betweenness\"] = float(btw.get(n_, 0.0))\n",
        "        G.nodes[n_][\"eigenvector\"] = float(eig.get(n_, 0.0))\n",
        "\n",
        "    from networkx.algorithms.community import louvain_communities, greedy_modularity_communities\n",
        "    try:\n",
        "        parts = louvain_communities(G, weight=\"weight\", seed=42)\n",
        "    except Exception:\n",
        "        parts = list(greedy_modularity_communities(G, weight=\"weight\"))\n",
        "    palette = [(244,67,54),(33,150,243),(76,175,80),(255,193,7),(156,39,176),(255,87,34),(0,188,212),(121,85,72),(63,81,181)]\n",
        "    comm_of = {n_:i for i,comm in enumerate(parts) for n_ in comm}\n",
        "    for n_ in G.nodes():\n",
        "        i = comm_of.get(n_, 0)\n",
        "        r,g,b = palette[i % len(palette)]\n",
        "        size = 10 + 4*math.sqrt(max(0.0, strength.get(n_,0)))\n",
        "        G.nodes[n_][\"community\"] = int(i)\n",
        "        G.nodes[n_][\"viz\"] = {\"color\":{\"r\":r,\"g\":g,\"b\":b}, \"size\":float(size)}\n",
        "\n",
        "    pos = nx.spring_layout(G, weight=\"weight\", seed=42)\n",
        "    for n_, (x,y) in pos.items():\n",
        "        G.nodes[n_].setdefault(\"viz\", {})\n",
        "        G.nodes[n_][\"viz\"][\"position\"] = {\"x\": float(x*1000), \"y\": float(y*1000), \"z\": 0.0}\n",
        "\n",
        "    for u,v,d in G.edges(data=True):\n",
        "        d.setdefault(\"viz\", {})\n",
        "        d[\"viz\"][\"thickness\"] = max(1.0, float(d.get(\"weight\",1)))\n",
        "\n",
        "    nx.write_gexf(G, path, encoding=\"utf-8\")\n",
        "    print(f\"GEXF écrit: {path} — {G.number_of_nodes()} nœuds / {G.number_of_edges()} arêtes\")"
      ],
      "metadata": {
        "id": "J1ZHHVQoBALy"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les cellules qui suivent sont simplement les appels des fonctions créées plus haut."
      ],
      "metadata": {
        "id": "7kMZh-zlDJ2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ents = collect_entities(full_text, keep_labels={\"PER\"}, tokenizer=tok, ner=ner)"
      ],
      "metadata": {
        "id": "QKiaQXbvBEmE"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in ents:\n",
        "    e[\"key\"] = canonicalize_entity(e[\"text\"])"
      ],
      "metadata": {
        "id": "daTStzdeBMGf"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_all   = Counter(e[\"key\"] for e in ents)\n",
        "merge_map  = build_merge_map(set(e[\"key\"] for e in ents), freq_all, min_len=MIN_WORD_LEN, max_run=3)\n",
        "for e in ents:\n",
        "    e[\"key2\"] = merge_map.get(e[\"key\"], e[\"key\"])"
      ],
      "metadata": {
        "id": "ctO_2TfZBRSZ"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Avant fusion:\", len(set(e[\"key\"] for e in ents)),\n",
        "      \"Après fusion:\", len(set(e[\"key2\"] for e in ents)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzeukV7ZFy4o",
        "outputId": "935a1454-feb4-436d-f1cf-7f72a2114d64"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avant fusion: 837 Après fusion: 569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq_merged = Counter(e[\"key2\"] for e in ents)\n",
        "ents_filt   = [e for e in ents if freq_merged[e[\"key2\"]] >= MIN_FREQ]\n",
        "print(f\"Labels gardés (≥{MIN_FREQ}) :\", sum(v>=MIN_FREQ for v in freq_merged.values()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOKdXw7jBUn7",
        "outputId": "8fbfefae-597c-450f-da21-608fae872265"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels gardés (≥5) : 96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ents_filt, _words = attach_token_index_to_entities(normalize_unicode(full_text), ents_filt)"
      ],
      "metadata": {
        "id": "ABCuZcRgBXYe"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "co = compute_cooccurrence(ents_filt, window_size_words=WINDOW_SIZE, key_fn=lambda e: e[\"key2\"])"
      ],
      "metadata": {
        "id": "P9e5Lt4uBaGu"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities, cooccurrence_matrix = cooccurrence_to_matrix(co)"
      ],
      "metadata": {
        "id": "aHJGKliIGISm"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = build_graph_from_matrix_safe(entities, cooccurrence_matrix, edge_min=EDGE_MIN)\n",
        "enrich_and_write_gexf(G, path=\"network.gexf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYIkdmGeGKWU",
        "outputId": "243cf0b6-f4bd-43c6-868f-156192a6ac5e"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GEXF écrit: network.gexf — 95 nœuds / 579 arêtes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_pairs = [(k, v) for k, v in merge_map.items() if k != v]\n",
        "print(\"Exemples de fusions :\", merged_pairs[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSNxzXMvGRCe",
        "outputId": "f8540a83-7613-46bb-b708-4c20eb178797"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exemples de fusions : [('Ὀδυσσεύ', 'Ὀδυσσεύς'), ('Ὀδυσσε', 'Ὀδυσσεύς'), ('ΟΔΥΣΣΕΙΑΣ', 'Ὀδυσσεύς'), ('ἐ Ὀδυσσεύς', 'Ὀδυσσεύς'), ('Ὀδυσσήιος', 'Ὀδυσσεύς'), ('Τηλέμαχ', 'Τηλέμαχος'), ('Τηλεμάχ', 'Τηλέμαχος'), ('Τηλέμα', 'Τηλέμαχος'), ('Τηλεμός', 'Τηλέμαχος'), ('Τηλέμαξ', 'Τηλέμαχος'), ('ἴς Τηλέμαχος', 'Τηλέμαχος'), (\"Τηλέμαχ'\", 'Τηλέμαχος'), ('ἐά Τηλέμαχος', 'Τηλέμαχος'), ('Πηνελόπις', 'Πηνελόπεια'), ('Πηνελοπεύς', 'Πηνελόπεια'), ('Ἀντίνοος', 'Ἀλκίνοος'), ('Ἀλκίνος', 'Ἀλκίνοος'), ('Ἀλκίνων', 'Ἀλκίνοος'), ('̓ἀλκίνος', 'Ἀλκίνοος'), ('Ἀλκινός', 'Ἀλκίνοος')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Juste quelques infos sur le graphe que vous allez obtenir.\n",
        "<br>**Couleur** du nœud : on applique Louvain (détection de communautés sur le graphe pondéré) puis on assigne une couleur par communauté avec une palette fixe dans le code.\n",
        "\n",
        "<br>L'index de communauté est enregistré dans l'attribut community du nœud.\n",
        "\n",
        "<br>La couleur affichée correspond à cet index (avec un seed=42 pour que ça reste stable d'un run à l'autre, si le graphe ne change pas).\n",
        "\n",
        "<br>S'il y a plus de 9 communautés, les couleurs recyclent (deux communautés peuvent partager la même teinte).\n",
        "\n",
        "<br>**Taille** du nœud : proportionnelle à la force (strength = degré pondéré), via size = 10 + 4*sqrt(strength) (dans viz.size).\n",
        "\n",
        "<br>**Épaisseur** des arêtes : proportionnelle au poids (cooccurrences), via viz.thickness."
      ],
      "metadata": {
        "id": "HITvIlI1JhlS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IHjfSsYSGuoN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}